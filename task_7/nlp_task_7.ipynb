{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 二维卷积层："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    卷积神经网络是含有卷积层的神经网络。二维卷积层具有高和宽两个空间维度。二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。\n",
    "    卷积窗口形状为 p×q 的卷积层称为 p×q 卷积层。同样， p×q 卷积或 p×q 卷积核说明卷积核的高和宽分别为 p 和 q 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def corr2d(X, K):  # 本函数已保存在d2lzh包中方便以后使用\n",
    "    h, w = K.shape\n",
    "    Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[19. 25.]\n",
       " [37. 43.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = nd.array([[0, 1], [2, 3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。在二维最大池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。当池化窗口滑动到某一位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = nd.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[4. 5.]\n",
       " [7. 8.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n",
    "\n",
    "卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5×5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为2×2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。\n",
    "\n",
    "卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import time\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成\n",
    "        # (批量大小, 通道 * 高 * 宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 24, 24)\n",
      "pool0 output shape:\t (1, 6, 12, 12)\n",
      "conv1 output shape:\t (1, 16, 8, 8)\n",
      "pool1 output shape:\t (1, 16, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\user\\.mxnet\\datasets\\fashion-mnist\\train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading C:\\Users\\user\\.mxnet\\datasets\\fashion-mnist\\train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading C:\\Users\\user\\.mxnet\\datasets\\fashion-mnist\\t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading C:\\Users\\user\\.mxnet\\datasets\\fashion-mnist\\t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu():  # 本函数已保存在d2lzh包中方便以后使用\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "ctx = try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中\n",
    "# 描述\n",
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc_sum, n = nd.array([0], ctx=ctx), 0\n",
    "    for X, y in data_iter:\n",
    "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx).astype('float32')\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh包中方便以后使用\n",
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs):\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu(0)\n",
      "epoch 1, loss 2.3203, train acc 0.102, test acc 0.152, time 25.9 sec\n",
      "epoch 2, loss 1.9456, train acc 0.248, test acc 0.575, time 25.5 sec\n",
      "epoch 3, loss 0.9640, train acc 0.618, test acc 0.676, time 25.0 sec\n",
      "epoch 4, loss 0.7541, train acc 0.705, test acc 0.735, time 25.6 sec\n",
      "epoch 5, loss 0.6738, train acc 0.733, test acc 0.751, time 25.4 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 5\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data('C:/Users/user/.keras/datasets/imdb.npz',num_words=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> shown in australia as <UNK> this incredibly bad movie is so bad that you become <UNK> and have to watch it to the end just to see if it could get any worse and it does the storyline is so predictable it seems written by a high school <UNK> class the sets are pathetic but marginally better than the <UNK> and the acting is wooden br br the infant <UNK> seems to have been stolen from the props <UNK> of <UNK> <UNK> there didn't seem to be a single original idea in the whole movie br br i found this movie to be so bad that i laughed most of the way through br br malcolm mcdowell should hang his head in shame he obviously needed the money\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          240000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 240,289\n",
      "Trainable params: 240,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = 15000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "#model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.train.AdamOptimizer(),#tf.train.AdamOptimizer() =>tf.optimizers.Adam()\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 3s 177us/step - loss: 0.7561 - acc: 0.5003 - val_loss: 0.6923 - val_acc: 0.5163\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.7141 - acc: 0.5097 - val_loss: 0.6908 - val_acc: 0.5358\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.7004 - acc: 0.5117 - val_loss: 0.6897 - val_acc: 0.5527\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.6943 - acc: 0.5187 - val_loss: 0.6882 - val_acc: 0.5783\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 72us/step - loss: 0.6906 - acc: 0.5243 - val_loss: 0.6857 - val_acc: 0.6359\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.6865 - acc: 0.5533 - val_loss: 0.6836 - val_acc: 0.6627\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.6829 - acc: 0.5808 - val_loss: 0.6814 - val_acc: 0.6321\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 0.6798 - acc: 0.5901 - val_loss: 0.6777 - val_acc: 0.7154\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.6754 - acc: 0.6205 - val_loss: 0.6737 - val_acc: 0.7243\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.6705 - acc: 0.6347 - val_loss: 0.6683 - val_acc: 0.7520\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 74us/step - loss: 0.6647 - acc: 0.6510 - val_loss: 0.6617 - val_acc: 0.7652\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.6564 - acc: 0.6723 - val_loss: 0.6533 - val_acc: 0.7703\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.6469 - acc: 0.6804 - val_loss: 0.6438 - val_acc: 0.7607\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.6351 - acc: 0.6965 - val_loss: 0.6308 - val_acc: 0.7829\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.6223 - acc: 0.7099 - val_loss: 0.6151 - val_acc: 0.7954\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.6062 - acc: 0.7286 - val_loss: 0.5969 - val_acc: 0.7984\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.5861 - acc: 0.7461 - val_loss: 0.5772 - val_acc: 0.8123\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 72us/step - loss: 0.5674 - acc: 0.7556 - val_loss: 0.5562 - val_acc: 0.8182\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.5474 - acc: 0.7707 - val_loss: 0.5339 - val_acc: 0.8264\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.5260 - acc: 0.7834 - val_loss: 0.5127 - val_acc: 0.8333\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.5036 - acc: 0.7963 - val_loss: 0.4900 - val_acc: 0.8404\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.4841 - acc: 0.8109 - val_loss: 0.4695 - val_acc: 0.8466\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.4625 - acc: 0.8179 - val_loss: 0.4494 - val_acc: 0.8505\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.4437 - acc: 0.8296 - val_loss: 0.4305 - val_acc: 0.8546\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.4228 - acc: 0.8419 - val_loss: 0.4144 - val_acc: 0.8584\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.4109 - acc: 0.8435 - val_loss: 0.4022 - val_acc: 0.8622\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 65us/step - loss: 0.3938 - acc: 0.8545 - val_loss: 0.3876 - val_acc: 0.8646\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.3795 - acc: 0.8612 - val_loss: 0.3755 - val_acc: 0.8689\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 74us/step - loss: 0.3657 - acc: 0.8682 - val_loss: 0.3635 - val_acc: 0.8709\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 72us/step - loss: 0.3538 - acc: 0.8706 - val_loss: 0.3550 - val_acc: 0.8722\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.3435 - acc: 0.8780 - val_loss: 0.3466 - val_acc: 0.8739\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.3337 - acc: 0.8814 - val_loss: 0.3393 - val_acc: 0.8758\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.3255 - acc: 0.8865 - val_loss: 0.3325 - val_acc: 0.8771\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.3157 - acc: 0.8872 - val_loss: 0.3270 - val_acc: 0.8780\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.3098 - acc: 0.8872 - val_loss: 0.3229 - val_acc: 0.8790\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.2999 - acc: 0.8951 - val_loss: 0.3163 - val_acc: 0.8800\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.2919 - acc: 0.8977 - val_loss: 0.3121 - val_acc: 0.8818\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.2875 - acc: 0.9027 - val_loss: 0.3084 - val_acc: 0.8825\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 65us/step - loss: 0.2813 - acc: 0.8998 - val_loss: 0.3054 - val_acc: 0.8838\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.2705 - acc: 0.9085 - val_loss: 0.3010 - val_acc: 0.8853\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 18us/step\n",
      "[0.31559182459831236, 0.87404]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
